Here I have experimented with few advanced framework implementation on llm engineering.

1. Using Semantic Cache For llm Inference cost savings and fast retrival. Code 
