Here I have experimented with few advanced framework implementation on llm engineering.

1. Using Semantic Cache For llm Inference cost savings and fast retrival. Code 



# Semantic Cache Implementation

This repository provides a Proof of Concept (POC) for implementing a semantic caching system using advanced natural language processing (NLP) models and libraries. The project demonstrates the use of semantic similarity to enhance cache efficiency and reduce redundant computations in data-heavy applications.

## Features
- **Semantic Similarity Matching**: Leverages pre-trained models to determine the similarity between queries.
- **Cache Optimization**: Implements intelligent caching strategies for faster data retrieval.
- **Scalable Design**: Built with modern NLP frameworks for seamless scalability.

## Installation

To set up the project locally, follow these steps:
1. Clone this repository:
   git clone https://github.com/abhishekdodda/semantic-cache.git
2. cd semantic-cache
3. pip install -r requirements.txt

**Open the Jupyter Notebook**
jupyter notebook LLMs_semantic_cache.ipynb

**Dependencies**
The project requires the following libraries:
faiss-cpu
sentence_transformers
transformers

Ensure these are installed before running the notebook.

**Contributing**
Contributions are welcome! Please fork the repository and create a pull request for any enhancements or bug fixes.

**License**
This project is licensed under the MIT License.

**Contact**
For questions or support, please reach out to abhishek.dodda1@gmail.com
